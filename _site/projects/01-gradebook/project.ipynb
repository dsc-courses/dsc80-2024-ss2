{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"project.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 ‚Äì Gradebook üíØ\n",
    "\n",
    "## DSC 80, Summer 2024\n",
    "\n",
    "<!-- ### Checkpoint Due Date: Friday, April 12th (Questions 1-7) -->\n",
    "### Due Date: Wednesday, August 14th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Project 1! Be sure to read the instructions below carefully to understand how projects differ from labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the Project\n",
    "\n",
    "This Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems.\n",
    "\n",
    "* Like the lab, your coding work will be developed in the accompanying `project.py` file, that will be imported into the current notebook. This code will be autograded.\n",
    "* There is no manually-graded component to Project 1, so the only thing you will ever submit is `project.py`.\n",
    "* **For the Checkpoint, which is required, you only need to turn in a `project.py` containing solutions for Questions 1-7!**\n",
    "    - The \"Project 1 Checkpoint\" autograder on Gradescope does not thoroughly check your code ‚Äì it only runs the public tests on Questions 1-7 to make sure that you have completed them. There are no hidden tests for the checkpoint, and you will see your score upon submission. \n",
    "    - When you submit the final version of the project, however, we will use hidden tests to check your answers more thoroughly.\n",
    "    - Note that this means you will ultimately have to submit the project twice ‚Äì once to the \"Project 1 Checkpoint\" autograder (Questions 1-7 only), and once to the \"Project 1\" autograder (once you're fully done).\n",
    "- **Do not change the function names in `project.py` file!** The functions in `project.py` are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2024-ss2).\n",
    "- **To ensure that all of your work to be submitted is in `project.py`, we've included a script named `project-validation.py` in the project folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.** More details on its usage are given at the bottom of this notebook.\n",
    "- You are encouraged to write your own additional helper functions to solve the project, as long as they also end up in `project.py`.\n",
    "\n",
    "### Working with a Partner\n",
    "\n",
    "You may work together on projects (and projects only!) with a partner. If you work with a partner, you are both required to actively contribute to all parts of the project. You must both be working on the assignment at the same time together, either physically or virtually on a Zoom call. You are encouraged to follow the pair programming model, in which you work on just a single computer and alternate who writes the code and who thinks about the problems at a high level.\n",
    "\n",
    "In particular, you **cannot** split up the project and each work on separate parts independently.\n",
    "\n",
    "Note that if you do work with a partner, you and your partner must submit the Checkpoint together and the whole project together. See [here](https://dsc80.com/syllabus/#projects) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Assignment\n",
    "\n",
    "In this project, you'll work with the gradebook for CSD 18, a fictional data science course with 535 students co-taught by Professors Yutian and Dylan. You'll help Professors Yutian and Dylan compute the total course grade for every student in their course and analyze their students' performances throughout the quarter.\n",
    "\n",
    "---\n",
    "\n",
    "### Navigating the Project\n",
    "\n",
    "Click on the links below to navigate to different parts of the project. Note that Part 1 ‚Äì that is, Questions 1, 2, 3, 4, 5, 6, and 7 ‚Äì constitutes your Checkpoint submission.\n",
    "\n",
    "- [Part 1: Initial Calculations üî¢](#part1)\n",
    "    - [Question 1 ](#Question-1-(Checkpoint-Question))\n",
    "    - [Question 2 ](#Question-2-(Checkpoint-Question))\n",
    "    - [Question 3 ](#Question-3-(Checkpoint-Question))\n",
    "    - [Question 4 ](#Question-4-(Checkpoint-Question))\n",
    "    - [Question 5 ](#Question-5-(Checkpoint-Question))\n",
    "    - [Question 6 ](#Question-6-(Checkpoint-Question))\n",
    "    - [Question 7 ](#Question-7-(Checkpoint-Question))\n",
    "- [Part 2: Redemption üôè](#part2)\n",
    "    - [Question 8](#Question-8)\n",
    "    - [Question 9](#Question-9)\n",
    "    - [Question 10](#Question-10)\n",
    "- [Part 3: Analysis üß†](#part3)\n",
    "    - [Question 11](#Question-11)\n",
    "    - [Question 12](#Question-12)\n",
    "    - [Question 13](#Question-13)\n",
    "\n",
    "<!--     - [‚úÖ Question 1 (Checkpoint Question)](#Question-1-(Checkpoint-Question))\n",
    "    - [‚úÖ Question 2 (Checkpoint Question)](#Question-2-(Checkpoint-Question))\n",
    "    - [‚úÖ Question 3 (Checkpoint Question)](#Question-3-(Checkpoint-Question))\n",
    "    - [‚úÖ Question 4 (Checkpoint Question)](#Question-4-(Checkpoint-Question))\n",
    "    - [‚úÖ Question 5 (Checkpoint Question)](#Question-5-(Checkpoint-Question))\n",
    "    - [‚úÖ Question 6 (Checkpoint Question)](#Question-6-(Checkpoint-Question))\n",
    "    - [‚úÖ Question 7 (Checkpoint Question)](#Question-7-(Checkpoint-Question)) -->\n",
    "---\n",
    "\n",
    "### The Syllabus\n",
    "\n",
    "Professor Yutian has taught this course several times, so the instructors decide to use her syllabus at the start of the quarter. (Note that this syllabus is **not** the same as the course syllabus for DSC 80 in Summer 2024)\n",
    "\n",
    "* **Lab assignments (20% total)**\n",
    "    - Each lab is worth the same amount, regardless of each lab's raw point total.\n",
    "    - The lowest lab is dropped.\n",
    "    - Each lab may be revised for up to (and including) one week after the deadline for a 10% penalty, for up to (and including) two weeks after the deadline for a 30% penalty, and beyond that for a 60% penalty. Such revisions are reflected in the `'Lateness'` columns in the gradebook.\n",
    "    - Labs also have a two hour grace period that needs to be factored in before assigning late penalties.\n",
    "    - Note that lateness penalties are not assessed for any other type of assignment ‚Äì that is, students can submit projects, checkpoints, and discussions late without penalty.\n",
    "* **Projects (30% total)** \n",
    "    - Each project consists of an autograded portion, and **possibly** a free response portion.\n",
    "    - The total points for a single project consist of the sum of the raw score of the two portions.\n",
    "    - Each project is worth the same amount, regardless of each project's raw point total.\n",
    "* **Checkpoints (2.5% total)**\n",
    "    - Each project checkpoint is worth the same amount, regardless of each project checkpoint's raw point total.\n",
    "* **Discussions (2.5% total)**\n",
    "    - Each discussion is worth the same amount, regardless of each discussion's raw point total.\n",
    "* **Midterm Exam (15%)**\n",
    "* **Final Exam (30%)**\n",
    "\n",
    "You will need to refer to this syllabus repeatedly throughout the project, and several questions will link you back to it.\n",
    "\n",
    "---\n",
    "\n",
    "### Generalization\n",
    "\n",
    "Your code only needs to work for courses that follow the syllabus above. That is, you may assume that the DataFrame `grades` looks **like** the given one in `data/grades.csv`.\n",
    "\n",
    "However, your code should work regardless of:\n",
    "- The numbers of labs, projects, discussions, and checkpoints in the course.\n",
    "- The number of students in the course.\n",
    "\n",
    "For instance, if CSD 18 is taught in a different quarter with more labs, fewer projects, and fewer students, your code should still work on a `grades.csv` from that quarter.\n",
    "\n",
    "You may assume the course components and the naming conventions are as given in `grades.csv`, and you may assume that the course has no more than 99 of any type of assignment.\n",
    "\n",
    "---\n",
    "\n",
    "### Putting Everything Together\n",
    "\n",
    "Here are a few remarks and tips for approaching Project 1, and projects more generally:\n",
    "\n",
    "1. If you are having trouble figuring out what a question is asking you to do, look at the big picture and try to understand what the current step is doing to contribute to this big picture. This may clarify what's being asked!\n",
    "1. These questions intentionally build off of each other and the final result matters! In fact, you can \"get a question correct,\" but only receive partial credit for it because a previous answer was wrong.\n",
    "    - Credit for a question will typically receive partial credit based on *how close* your answer is to correct (as well as some credit for a solution in the correct form). \n",
    "    - You should try to assess your answer to each question based on what you understand of the data. This might involve writing extensive code (that isn't turned in) just to check your work! Suggestions on checking your work are given in the assignment, but you should also think of your own ways of checking your work.\n",
    "    - As you do this project, think about the data from the perspective of the student (which should be easy to do, since you've used Gradescope before!)\n",
    "1. To test the correctness of your answers:\n",
    "     - Once you have implemented a particular function in `project.py`, you should test out your function in the notebook. In particular, you should inspect/analyze the output to assess its correctness.\n",
    "    - Run your functions on the main dataset (`grades`, and later `grades_combined` and `grades_analysis`) and ask yourself if the output *looks correct.*\n",
    "    - Run your functions on very small datasets (e.g. 1-5 row DataFrames that you construct by hand), calculate the expected output by hand, and see if the function output matches (this *is* unit-testing your code with data).\n",
    "    * Run your functions on (large and small) samples of the dataset `grades`. Does your code break, or does it still run as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to load in the aforementioned `grades` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_fp = Path('data') / 'grades.csv'\n",
    "grades = pd.read_csv(grades_fp)\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tip***: The `grades` DataFrame has 101 columns, and you can't see them all right now. To get a feel for what all of the columns represent, you might consider opening `grades.csv` with a spreadsheet application, like Google Sheets or Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part1'></a>\n",
    "\n",
    "## Part 1: Initial Calculations üî¢\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "In Part 1, you'll compute students' letter grades in CSD 18 using [the syllabus](#The-Syllabus) provided above. As you'll see, this requires many steps. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### ‚úÖ Question 1 (Checkpoint Question) -->\n",
    "### Question 1 \n",
    "\n",
    "\n",
    "<a name='Question-1-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Complete the implementation of the function `get_assignment_names`, which takes in a DataFrame like `grades` and returns a dictionary with the following structure:\n",
    "- The keys are the general areas of [the syllabus](#The-Syllabus): `'lab'`, `'project'`, `'midterm'`, `'final'`, `'disc'`, and `'checkpoint'`.\n",
    "- The values are **lists** that contain all the assignment names of that type. For example, the lab assignments all have names of the form `'labXX'` where `XX` is a zero-padded two digit number. If the class has 5 labs, the returned dictionary's value for the `'lab'` key should be `['lab01', 'lab02', 'lab03', 'lab04', 'lab05']`.\n",
    "\n",
    "***Notes***: \n",
    "- Some of the column names in the DataFrame contain the assignment name in the zero-padded fashion requested; you should use this to your advantange when building the dictionary.\n",
    "- The point of this question is to familiarize you with the names of the columns in `grades`. Try to reuse your `get_assignment_names` function in future questions ‚Äì if you find yourself never using it again, you may be redoing work unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to compute each student's overall grade on the first type of assignment ‚Äì projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### ‚úÖ Question 2 (Checkpoint Question) -->\n",
    "### Question 2\n",
    "\n",
    "\n",
    "<a name='Question-2-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Complete the implementation of the function `projects_total`, which takes in a DataFrame like `grades` and returns a Series containing the total project grade for each student for the entire quarter, according to [the syllabus](#The-Syllabus). The output Series should contain values between 0 and 1.\n",
    "\n",
    "***Notes***:\n",
    "\n",
    "- If a student didn't turn in a particular project, what should their grade for it be? \n",
    "- Some projects have free response components that you need to account for when calculating the total points earned by a student and the max points possible for that project.\n",
    "    - For instance, let's say Tiffany earned 82 points on the autograded portion of Project 1 and 13 points on the free response portion. This means that her overall Project 1 grade should be:\n",
    "    $$\n",
    "        \\text{Project 1 Grade} = \\frac{82+13}{85+15} = 0.95\n",
    "    $$\n",
    "- Per [the syllabus](#The-Syllabus), students may submit projects (and checkpoints and discussions) late without penalty.\n",
    "- Do not include scores on checkpoint assignments in your calculations.\n",
    "- To check your work, try:\n",
    "    1. Calculating the total project scores for a few types of students by hand.\n",
    "    2. Calculating summary statistics for the whole class' performance on a few projects in particular and ensuring the results seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that projects are out of the way, you need to clean and process the lab grades. This will involve a bit more work than was necessary for projects. Specifically, you'll:\n",
    "- identify late submissions (Question 3), \n",
    "- compute normalized scores for each lab assignment, factoring in late penalties (Question 4), and \n",
    "- drop the lowest lab grade and compute a total lab score for each student (Question 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### ‚úÖ Question 3 (Checkpoint Question) -->\n",
    "### Question 3 \n",
    "\n",
    "\n",
    "<a name='Question-3-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Recall, per [the syllabus](#The-Syllabus), labs are the only assignment category for which late penalties are enforced:\n",
    "\n",
    ">  Each lab may be revised for up to (and including) one week after the deadline for a 10% penalty, for up to (and including) two weeks after the deadline for a 30% penalty, and beyond that for a 60% penalty. Such revisions are reflected in the `'Lateness'` columns in the gradebook.\n",
    "\n",
    "For labs, students have a **two hour grace period** after the deadline during which their submissions are counted as on time. The grace period only applies to the original deadline ‚Äì for instance, if a student submits one week and one hour late, their submission falls into the \"up to (and including) two weeks after the deadline\" category and they are assessed a 30% penalty.\n",
    "\n",
    "Your job is to adjust lab grades to penalize **truly** late submissions, factoring in the grace period. To adjust a student's grade, multiply their lab score by `1` (on time, factoring in the grace period), `0.9`, `0.7`, or `0.4`. We'll call these four numbers ‚Äì `1`, `0.9`, `0.7`, and `0.4` ‚Äì \"lateness multipliers.\" \n",
    "\n",
    "Complete the implementation of the function `lateness_penalty`, which takes in a Series containing information on how late each student turned in a particular lab, such as `grades['lab01 - Lateness (H:M:S)']`, and returns a Series containing each student's lateness multiplier for that lab. The only possible values in the returned Series should be `1.0`, `0.9`, `0.7`, and `0.4`.\n",
    "\n",
    "**Don't forget to factor in the grace period!** Remember, we will only be enforcing late penalties for labs, not for any other assignment category.\n",
    "\n",
    "**Note**: There is no grace period for real Gradescope!! Make sure you submit your assignments on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### ‚úÖ Question 4 (Checkpoint Question) -->\n",
    "### Question 4 \n",
    "\n",
    "<a name='Question-4-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Complete the implementation of the function `process_labs`, which takes in a DataFrame like `grades` and returns a DataFrame of processed lab scores. The returned DataFrame should:\n",
    "* have the same index as `grades`,\n",
    "* have one column for each lab assignment (e.g. `'lab01'`, `'lab02'`,..., `'lab09'`),\n",
    "* have values representing the final score for each lab assignment, adjusted for lateness and **normalized** to a score between 0 and 1.\n",
    "\n",
    "Remember to correctly handle the case where a student _doesn't_ turn in a lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### ‚úÖ Question 5 (Checkpoint Question) -->\n",
    "### Question 5 \n",
    "\n",
    "<a name='Question-5-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Complete the implementation of the function `lab_total`, which takes in a DataFrame returned by `process_labs` ‚Äì that is, a DataFrame that contains each student's score on each lab after lateness penalties ‚Äì and returns a Series containing the total lab grade for each student according to [the syllabus](#The-Syllabus) (i.e. with the lowest lab dropped). All values in the returned Series should be proportions between 0 and 1. \n",
    "\n",
    "For example, if CSD 18 only has 3 labs, and Aritra received lab scores of 20%, 90%, and 100% after lateness penalties, then your output Series should contain the value `0.95` for Aritra. This is because we drop the lowest score, and then compute the average of just 90% and 100%, which is 95%, or 0.95 as a proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that projects and labs are processed, we're almost ready to compute the letter grade of each student in CSD 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "<!-- ### ‚úÖ Question 6 (Checkpoint Question) -->\n",
    "\n",
    "<a name='Question-6-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "First, you need to compute each student's course grade, which results from adding their total grades in each course component according to the weights given in [the syllabus](#The-Syllabus).\n",
    "\n",
    "Complete the implementation of the function `total_points`, which takes in a DataFrame like `grades` and returns a Series containing each student's course grade. **Course grades should be proportions between 0 and 1.**\n",
    "\n",
    "***Notes***: \n",
    "\n",
    "- Don't repeat yourself when computing the checkpoint and discussion portions of the course.\n",
    "- Remember, only the lab portion of the course accounts for late assignments; you may assume all assignments in other portions are turned in without penalty.\n",
    "- Do the work by hand for a few students to check your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### ‚úÖ Question 7 (Checkpoint Question) -->\n",
    "### Question 7\n",
    "\n",
    "<a name='Question-7-(Checkpoint-Question)'></a>\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "How well did the students in CSD 18 do?\n",
    "\n",
    "#### `final_grades`\n",
    "\n",
    "Complete the implementation of the function `final_grades`, which takes in a Series of final course grades (as computed by `total_points` in Question 6) and returns a Series of letter grades as determined by the following cutoffs:\n",
    "\n",
    "| Letter Grade | Cutoff |\n",
    "|:--- | --- |\n",
    "| A | grade >= 0.9 |\n",
    "| B | 0.8 <= grade < 0.9 |\n",
    "| C | 0.7 <= grade < 0.8 |\n",
    "| D | 0.6 <= grade < 0.7 |\n",
    "| F | grade < 0.6 |\n",
    "\n",
    "***Note***: These cutoffs do not have pluses or minuses. **Do not round** anyone's course grade when determining their letter grade.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `letter_proportions`\n",
    "\n",
    "Complete the implementation of the function `letter_proportions`, which takes in a Series of final course grades (as computed by `total_points` in Question 6) and returns a Series containing the proportion of the class that received each letter grade. For instance, this Series might tell us that the proportion of the class receiving B's was 0.45, A's was 0.33, C's was 0.16, D's was 0.05, and F's was 0.01 (though these are made up numbers). The index of this Series should be letters, and the **values should be sorted in decreasing order**.\n",
    "\n",
    "***Notes***: \n",
    "\n",
    "- The values in your returned Series should add up to exactly `1.0`. If you are getting something close such as `0.99999`, that means there is an issue with your code in a function you implemented earlier.\n",
    "- **Do not round**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part2'></a>\n",
    "\n",
    "## Part 2: Redemption üôè\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "The syllabus we've used so far was put together by Professor Yutian, who has taught CSD 18 for several iterations. This was Professor Dylan's first time teaching CSD 18, and towards the end of the quarter he proposed a new idea to reward students for showing an improvement in their understanding of the earlier ideas in the course on the final exam. Specifically, here's what he proposed:\n",
    "\n",
    "- The instructors will identify the questions on the final exam that contain content that was also covered on the midterm exam. Call these \"redemption questions.\"\n",
    "- For each student, compute their \"raw redemption score\", which is the proportion of points available on redemption questions that they earned. If they did not take the final exam, their raw redemption score is 0.\n",
    "- Convert the class' raw redemption scores to z-scores, i.e. to standard units.\n",
    "- Convert the class' original midterm exam grades, as proportions, to z-scores.\n",
    "- If a student's raw redemption z-score is higher than their original midterm exam z-score, replace their original midterm exam score with one that has a z-score equal to their raw redemption z-score. This is done by converting their raw redemption z-score back to a midterm grade proportion using the standard deviation and mean of the midterm exam.\n",
    "- If not, leave their original midterm exam score as-is. **Note that this policy can only increase a student's midterm exam score (and, hence, their total course grade), not decrease!**\n",
    "\n",
    "As a refresher from [DSC 10](https://dsc-courses.github.io/dsc10-2022-fa/resources/lectures/lec21/lec21.html#Standard-units), to convert a sequence of numbers to z-scores, or standard units, we use the following formula:\n",
    "\n",
    "$$z(x_i) = \\frac{x_i - \\text{mean of } x}{\\text{SD of }x}$$\n",
    "\n",
    "To illustrate this redemption policy, let's look at a concrete example.\n",
    "\n",
    "- Suppose the final exam was worth 80 points. 55 of these points came from Questions 2, 4, 6, 8, and 9, which were the redemption questions. The class' mean score on just the redemption questions was 0.8, with a standard deviation of 0.15.\n",
    "- Suppose the midterm exam was worth 70 points. The class' mean score on the midterm exam was 0.6, with a standard deviation of 0.25.\n",
    "- Jasmine, a student in the course, earned a $\\frac{74}{80}$ on the final exam, including a $\\frac{51}{55}$ on the redemption questions, and a $\\frac{53}{70}$ on the midterm exam. Then:\n",
    "    - Her raw redemption score is $\\frac{51}{55}$, and her redemption z-score is $\\frac{\\frac{51}{55} - 0.8}{0.15} \\approx 0.8485$.\n",
    "    - Her midterm z-score is $\\frac{\\frac{53}{70} - 0.6}{0.25} \\approx 0.6286$.\n",
    "    - Since her redemption z-score, $0.8485$, is greater than her midterm z-score, $0.6286$, her midterm exam score of $\\frac{53}{70} \\approx 0.7571$ will be replaced with:\n",
    "    \n",
    "    $$\\text{Jasmine's redemption z-score} \\cdot \\text{class' midterm SD} + \\text{class' midterm mean} \\approx 0.8485 \\cdot 0.25 + 0.6 = \\boxed{0.8121}$$\n",
    "\n",
    "Now, your job will be to implement this redemption policy and recompute each student's total course points. Before proceeding, you should think about _why_ Professor Dylan has chosen to implement the redemption policy in terms of z-scores, rather than in terms of raw scores.\n",
    "\n",
    "A few more things to consider:\n",
    "- We rounded in the example above, but you should not round at any point in this part.\n",
    "- After redemption, midterm exam grades should be capped at 1 (as a proportion), i.e. 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that CSVs like `grades.csv` don't actually contain all of the information you'll need to implement this policy. For instance, `grades` only contains each student's total final exam grade, but not the number of points they earned on each question.\n",
    "\n",
    "That information will come from another source. For the students whose grades are in `grades`, the CSV `data/final_exam_breakdown.csv` contains the number of points each student earned on each question of CSD 18's final exam. Run the cell below to load this CSV in as a DataFrame named `final_breakdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_breakdown_fp = Path('data') / 'final_exam_breakdown.csv'\n",
    "final_breakdown = pd.read_csv(final_breakdown_fp)\n",
    "final_breakdown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `final_breakdown` has the same number of rows as `grades`, but a different number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_breakdown.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that student `'A99381181'` has a score of `NaN` for each question because they did not take the final exam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades.loc[grades['PID'] == 'A99381181', 'Final']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "#### `raw_redemption`\n",
    "\n",
    "Complete the implementation of the function `raw_redemption`, which takes in a DataFrame like `final_breakdown` and a list of integers, corresponding to the question numbers for \"redemption questions.\" The function should return a DataFrame with two columns:\n",
    "- `'PID'`, the PID for each student in `final_breakdown`.\n",
    "- `'Raw Redemption Score'`, which is the proportion of points each student earned, when only considering redemption questions.\n",
    "\n",
    "For example, suppose `example_breakdown` is as follows:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>PID</th>\n",
    "      <th>Question 1 (6.0 pts)</th>\n",
    "      <th>Question 2 (3.0 pts)</th>\n",
    "      <th>Question 3 (1.0 pts)</th>\n",
    "      <th>Question 4 (4.5 pts)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>A99706914</td>\n",
    "      <td>6</td>\n",
    "      <td>3</td>\n",
    "      <td>1</td>\n",
    "      <td>4.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>A99237411</td>\n",
    "      <td>2</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>4.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>A99489712</td>\n",
    "      <td>4</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>4.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "`raw_redemption(example_breakdown, [1, 3])` should return the following DataFrame:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>PID</th>\n",
    "      <th>Raw Redemption Score</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>A99706914</td>\n",
    "      <td>1.000000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>A99237411</td>\n",
    "      <td>0.428571</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>A99489712</td>\n",
    "      <td>0.571429</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "***Notes***:\n",
    "- **Assume that for each question in `final_breakdown`, at least one student received a perfect score.**\n",
    "- Assume that the input DataFrame will be in the same format as `final_breakdown`, in that the column at position 0 will be labeled `'PID'`, the column at position 1 will contain scores for Question 1, the column at position 2 will contain scores for Question 2, and so on.\n",
    "- If a student didn't take the final, their raw redemption score should be 0.\n",
    "- Again, do not round.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `combine_grades`\n",
    "\n",
    "Then, complete the implementation of the function `combine_grades`, which takes in a DataFrame like `grades` and a DataFrame like the one returned by `raw_redemption`. The function should return a new DataFrame with all the columns from `grades`, plus a new column labelled `'Raw Redemption Score'` which contains the raw redemption score for each student.\n",
    "\n",
    "***Hint***: We cannot directly add the `'Raw Redemption Score'` from the redemption DataFrame to the `grades` DataFrame, as the `'PID'` columns in the two DataFrames won't necessarily match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our particular offering of CSD 18, the redemption questions on the final exam were Questions 1, 2, 3, 7, 9, and 12. Run the cell below to define a new DataFrame named `grades_combined` that results from calling the above two functions on grades from this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_combined = combine_grades(grades, raw_redemption(final_breakdown, [1, 2, 3, 7, 9, 12]))\n",
    "grades_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Now that we have all of our information about each student in one DataFrame, we can compute their z-score on both the original midterm exam and the redemption questions on the final exam.\n",
    "\n",
    "#### `z_score`\n",
    "\n",
    "Complete the implementation of the function `z_score`, which takes in a Series of numbers and returns a Series in which all elements are converted to z-scores. As a reminder, to convert a sequence of numbers to z-scores, or standard units, we use the following formula:\n",
    "\n",
    "$$z(x_i) = \\frac{x_i - \\text{mean of } x}{\\text{SD of }x}$$\n",
    "\n",
    "***Notes***:\n",
    "\n",
    "- Make sure to set the `ddof=0` in whichever method or function you use to compute standard deviation. `numpy` and `pandas` both use different default denominators when computing standard deviation. (`ddof=0` computes the \"population\" standard deviation and `ddof=1` computes the \"sample\" standard deviation.)\n",
    "- Do **not** fill null values ‚Äì that is, if a value in the input Series is `NaN`, its value in the output Series should also be `NaN`. (Depending on how you implement `z_score`, this may happen automatically.)\n",
    "    - Address null midterm scores in `add_post_redemption`, not `z_score`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `add_post_redemption`\n",
    "\n",
    "Complete the implementation of the function `add_post_redemption`, which takes in a DataFrame like `grades_combined` and returns a DataFrame with all the columns from `grades_combined` in addition to two new columns:\n",
    "- `'Midterm Score Pre-Redemption'`, which contains each student's midterm exam score as a proportion between 0 and 1 **before** redemption.\n",
    "- `'Midterm Score Post-Redemption'`, which containing each student's midterm exam score **after** the redemption policy has been applied, again as a proportion between 0 and 1.\n",
    "\n",
    "You can use your `z_score` function to compute the z-scores of each student's original midterm exam grades and raw redemption scores. **Note that there are students who didn't take the midterm; such students need to have their `NaN` scores fixed prior to calculating their pre-redemption z-scores**, otherwise, you may end up incorrectly giving them `NaN` post-redemption midterm scores. None of the redemption z-scores should be `NaN`, since you handled null values in your implementation of `raw_redemption`.\n",
    "\n",
    "If it's not clear, **computing the `'Midterm Score Post-Redemption'` column is the most complicated part of this question**. Make sure you understand how the redemption policy for CSD 18 works before approaching this question. If you need to refresh your understanding, re-read the instructions at the start of [Part 2](#part2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Now, we're equipped to re-compute each student's course grade after the redemption policy.\n",
    "\n",
    "#### `total_points_post_redemption`\n",
    "\n",
    "Complete the implementation of the function `total_points_post_redemption`, which takes in a DataFrame like `grades_combined` and returns a Series containing each student's course grade after redemption. As a refresher, **course grades should be proportions between 0 and 1.**\n",
    "\n",
    "You should not have to repeat any of your calculations for assignments other than the midterm exam ‚Äì use your output from `total_points` and adjust it. Remember that, per [the syllabus](#The-Syllabus), the midterm exam is worth 15%.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `proportion_improved`\n",
    "\n",
    "Finally, complete the implementation of the function `proportion_improved`, which takes in a DataFrame like `grades_combined` and returns the **proportion of students in the class whose letter grade increased** due to the redemption policy.\n",
    "\n",
    "***Hints***:\n",
    "- If you've implemented everything correctly, `proportion_improved(grades_combined)` should evaluate to a proportion between 0.07 and 0.12.\n",
    "- Remember, it is impossible for a student's letter grade to decrease due to the redemption policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Thanks to your implementation of the redemption policy, a sizeable fraction of CSD 18 students saw their letter grades improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='part3'></a>\n",
    "\n",
    "## Part 3: Analysis üß†\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "\n",
    "Now that we have students' letter grades before and after redemption, it's time to analyze how the class performed overall. First, because we're going to use them frequently in this part, we'll add a few extra columns to `grades_combined` and call the resulting DataFrame `grades_analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_analysis = grades_combined.assign(**{\n",
    "    'Total Points Pre-Redemption': total_points(grades_combined),\n",
    "    'Letter Grade Pre-Redemption': final_grades(total_points(grades_combined)),\n",
    "    'Total Points Post-Redemption': total_points_post_redemption(grades_combined),\n",
    "    'Letter Grade Post-Redemption': final_grades(total_points_post_redemption(grades_combined))\n",
    "})\n",
    "grades_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that `grades_analysis` has a `'Section'` column that we haven't yet touched. There are 30 unique values in the `'Section'` column ‚Äì `'A01'`, `'A02'`, ..., `'A30'`, corresponding to the 30 different discussion sections the students CSD 18 were enrolled in. Discussion sections and discussion assignments have nothing to do with one another, for the purposes of calculating grades, and moving forward, we'll refer to these just as \"sections.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_analysis['Section'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_analysis['Section'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of our analysis in this part will pertain to how students in different sections performed in CSD 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "#### `section_most_improved`\n",
    "\n",
    "Complete the implementation of the function `section_most_improved`, which takes in a DataFrame like `grades_analysis` and returns the section in which **the greatest proportion of students had their letter grades increase due to the redemption policy**. For example, if 48\\% of students in section `'A25'` had their letter grades increase due to the redemption policy, and no other section had more than 48\\% of students increase, then `section_most_improved` should return `'A25'`. \n",
    "\n",
    "If there is a tie, return any one of the sections.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `top_sections`\n",
    "\n",
    "Complete the implementation of the function `top_sections`, which takes in a DataFrame like `grades_analysis`, a float `t` between 0 and 1, and an integer `n`, and returns **an array containing the sections in which at least `n` students earned a raw score of at least `t` on the final exam**. The section names in the returned array should be sorted in alphanumeric order.\n",
    "\n",
    "For example, `top_sections(grades_analysis, 0.75, 10)` should return an array of the sections in which at least 10 students scored at least 75% on the final exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "Complete the implementation of the function `rank_by_section`, which takes in a DataFrame like `grades_analysis` and returns a DataFrame describing **students' _ranks_ based on total points (post-redemption) for each section**.\n",
    "\n",
    "Specifically, the DataFrame should have `n` rows that describe the rank ‚Äì indexed `1`, `2`, ..., `n` (where `n` is the number of students in the largest section), in that order ‚Äì and 30 columns ‚Äì `'A01'`, `'A02'`, ..., `'A30'`, in that order. **The entry in row `r` and column `s` should correspond to the PID of the student who had the `r`th most total points in section `s`, after redemption.** For sections that have fewer than `n` students, fill the extra entries in those columns with **empty strings**. There might exist ties for students with total points of 0.\n",
    "\n",
    "For instance, suppose there were only four sections, and the largest section had five students. The DataFrame returned by `rank_by_section` might look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>Section</th>\n",
    "      <th>A01</th>\n",
    "      <th>A02</th>\n",
    "      <th>A03</th>\n",
    "      <th>A04</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Section Rank</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>A99404117</td>\n",
    "      <td>A99318825</td>\n",
    "      <td>A99093358</td>\n",
    "      <td>A99339719</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>A99477753</td>\n",
    "      <td>A99396913</td>\n",
    "      <td>A99933171</td>\n",
    "      <td>A99082089</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td></td>\n",
    "      <td>A99159214</td>\n",
    "      <td>A99164028</td>\n",
    "      <td>A99950565</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td></td>\n",
    "      <td>A99322859</td>\n",
    "      <td></td>\n",
    "      <td>A99715029</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td></td>\n",
    "      <td>A99739120</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Note that the PIDs in your DataFrame will be different than those above; also note that your DataFrame may have a different string where the example has `'Section Rank'`, and that's fine.\n",
    "\n",
    "***Hints***: \n",
    "- Our solution used `groupby` with a helper function, and then `pivot` on the result. This is a tricky problem ‚Äì work through it one step at a time.\n",
    "\n",
    "- Try to use `.sort_values` rather than `.rank` in this question. This is because ties are assigned the mean of the ranks of the ties by default if you use `.rank`. For more information, please refer to `.rank` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rank.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "([return to the outline](#Navigating-the-Project))\n",
    "\n",
    "To wrap up, let's _visualize_ how the students in each section of CSD 18 performed.\n",
    "\n",
    "Complete the implementation of the function `letter_grade_heat_map`, which takes in a DataFrame like `grades_analysis` and returns a `plotly` figure object containing a heatmap describing **the distribution of letter grades (post-redemption) for each section**.\n",
    "\n",
    "Specifically, the heatmap should have 5 rows ‚Äì `'A'`, `'B'`, `'C'`, `'D'`, and `'F'`, in that order ‚Äì and 30 columns ‚Äì `'A01'`, `'A02'`, ..., `'A30'`, in that order. **The color of the square in row `g` and column `s` should correspond to the proportion of students in section `s` who earned a letter grade (post-redemption) of `g`.**\n",
    "\n",
    "To create your figure, you'll use the `px.imshow` function and provide several arguments. This [`plotly` article](https://plotly.com/python/imshow/) will be extremely helpful.\n",
    "\n",
    "Here are some additional requirements to get full credit for your heatmap:\n",
    "- Set the color scale to be something other than the default. Note that in this heatmap, you should use a sequential color scheme, which means that the intensity of the color assigned to a square is proportional to the value being plotted for that square (e.g. darker colors should correspond to larger proportions and lighter colors should correspond to smaller proportions, or vice versa). Read more about the theory of sequential and diverging color schemes [here](https://blog.datawrapper.de/diverging-vs-sequential-color-scales/).\n",
    "- Set the title of the plot to `'Distribution of Letter Grades by Section'`.\n",
    "\n",
    "An example plot that satisfies all of these conditions is shown below, though we encourage you to customize yours within the confines above. Can you change the font?\n",
    "\n",
    "<img src=\"data/heatmap-example.png\" width=100%>\n",
    "\n",
    "It's fine if your x-axis labels are rotated.\n",
    "\n",
    "Remember to return the figure object itself. That is, somewhere in your code you will have `fig = px.imshow(...)`; make sure to also `return fig`.\n",
    "\n",
    "***Hint***: Most of the work in this question is creating the DataFrame to call `px.imshow` on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see your heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the result, and don't change this cell --- it is needed for the tests.\n",
    "fig = letter_grade_heat_map(grades_analysis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you've finished Project 1! üéâ\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `project.py` ‚Äì this notebook should not be uploaded because there are no manually-graded questions in this project.\n",
    "\n",
    "To ensure that all of the work you want to submit is in `project.py`, we've included a script named `project-validation.py` in the project folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.\n",
    "\n",
    "Once you've finished the project, you should open the command line and run, in the directory for this project:\n",
    "\n",
    "```\n",
    "python project-validation.py\n",
    "```\n",
    "\n",
    "**This will run all of the `grader.check` cells that you see in this notebook, but only using the code in `project.py` ‚Äì that is, it doesn't look at any of the code in this notebook. If all of your `grader.check` cells pass in this notebook but not all of them pass in your command line with the above command, then you likely have code in your notebook that isn't in your `project.py`!**\n",
    "\n",
    "You can also use `project-validation.py` to test individual questions. For instance,\n",
    "\n",
    "```\n",
    "python project-validation.py q1 q4 q7 q8\n",
    "```\n",
    "\n",
    "will run the `grader.check` cells for Questions 1, 4, 7, and 8 ‚Äì again, only using the code in `project.py`.\n",
    "\n",
    "Once `python project-validation.py` shows that you're passing all test cases, you're ready to submit your `project.py` (and only your `project.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> out['final'] == ['Final']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> set(out.keys()) == {'lab', 'project', 'midterm', 'final', 'disc', 'checkpoint'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = get_assignment_names(grades)\n>>> 'project02' in out['project']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q10": {
     "name": "q10",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> total_points_post_redemption(grades_combined).dtypes == float\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(proportion_improved(grades_combined), float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0.07 < proportion_improved(grades_combined) < 0.12\nTrue",
         "failure_message": "Check proportion_improved with public grades_combined",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11": {
     "name": "q11",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> section_improved = section_most_improved(grades_analysis)\n>>> type(section_improved) == str\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> type(top_sections(grades_analysis, 0.75, 10)) == np.ndarray\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q12": {
     "name": "q12",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> section_rank = rank_by_section(grades_analysis)\n>>> type(section_rank) == pd.core.frame.DataFrame and section_rank.shape == (26, 30)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> section_rank = rank_by_section(grades_analysis)\n>>> section_rank.isna().sum().sum() == 0 and (section_rank == '').sum().sum() > 0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q13": {
     "name": "q13",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> str(type(fig)) == \"<class 'plotly.graph_objs._figure.Figure'>\" and fig.data[0].type == 'heatmap'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> fig.layout['title']['text'] == 'Distribution of Letter Grades by Section'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> fig.layout.coloraxis.colorscale != ((0.0, '#0d0887'),\n...  (0.1111111111111111, '#46039f'),\n...  (0.2222222222222222, '#7201a8'),\n...  (0.3333333333333333, '#9c179e'),\n...  (0.4444444444444444, '#bd3786'),\n...  (0.5555555555555556, '#d8576b'),\n...  (0.6666666666666666, '#ed7953'),\n...  (0.7777777777777778, '#fb9f3a'),\n...  (0.8888888888888888, '#fdca26'),\n...  (1.0, '#f0f921'))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> fig.layout.yaxis.title.text is not None\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = projects_total(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = projects_total(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = lateness_penalty(grades['lab01 - Lateness (H:M:S)'])\n>>> isinstance(out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = lateness_penalty(grades['lab01 - Lateness (H:M:S)'])\n>>> set(out.unique()) <= {1.0, 0.9, 0.7, 0.4}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = process_labs(grades)\n>>> out.columns.tolist() == ['lab%02d' % x for x in range(1, 10)]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = process_labs(grades)\n>>> np.all((0.60 <= out.mean()) & (out.mean() <= 0.90))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cols = 'lab01 lab02 lab03'.split()\n>>> processed = pd.DataFrame([[0.2, 0.90, 1.0]], index=[0], columns=cols)\n>>> np.isclose(lab_total(processed), 0.95).all()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = total_points(grades)\n>>> np.all((0 <= out) & (out <= 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = total_points(grades)\n>>> 0.7 < out.mean() < 0.9\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = final_grades(pd.Series([0.92, 0.81, 0.41]))\n>>> np.all(out == ['A', 'B', 'F'])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> total = total_points(grades)\n>>> out = letter_proportions(total)\n>>> np.all(out.index == ['B', 'C', 'A', 'D', 'F'])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> total = total_points(grades)\n>>> out = letter_proportions(total)\n>>> out.sum() == 1.0\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = raw_redemption(final_breakdown, [1])\n>>> 'PID' in out.columns and 'Raw Redemption Score' in out.columns\nTrue",
         "failure_message": "Check the column names of the DataFrame returned by raw_redemption!",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = combine_grades(grades, raw_redemption(final_breakdown, [1]))\n>>> out.shape == (535, 102) and 'Raw Redemption Score' in out.columns\nTrue",
         "failure_message": "Check the shape of the DataFrame returned by combine_grades!",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> test_ls = [27, 64, 156, 347]\n>>> np.allclose(raw_redemption(final_breakdown, [2, 3, 4, 5, 6]).iloc[test_ls][\"Raw Redemption Score\"], \n...             [0.805556, 0.694444, 1.000000, 0.722222])\nTrue",
         "failure_message": "Double check your implementation for this question as it can affect your result in later sections",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = z_score(grades['Final'])\n>>> out.dtype == float and out.shape[0] == grades.shape[0]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = add_post_redemption(grades_combined)\n>>> 'Midterm Score Pre-Redemption' in out and 'Midterm Score Post-Redemption' in out\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out = add_post_redemption(grades_combined)\n>>> np.any(np.isclose(out['Midterm Score Pre-Redemption'], out['Midterm Score Post-Redemption']))\nTrue",
         "failure_message": "Not all students' midterm scores should change!",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
